{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## PoliStance: Supervised Training\n",
        "\n",
        "This tutorial demonstrates how to train an NLI classifier as a supervised classifier. It uses the PoliStance model on the HuggingFace Hub, a DeBERTAv3 model trained for political stance classification. The base model should be sufficient for such tasks, although the large model may provide a benefit in instances with a low number of training samples."
      ],
      "metadata": {
        "id": "Uuaw8-qpn8S6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "euVo50_Lom-_"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset, DatasetDict # The datasets library allows us to import the data directly from the huggingface hub and puts it in an efficient format for training\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer # Transformers has the classes and functions for training the model\n",
        "import torch # Transformers is built on top of the pytorch library. This also allows us to interact with the GPU and check its availability.\n",
        "\n",
        "# numpy, pandas, and sklearn are used for data manipulation and performance metrics.\n",
        "import numpy as np\n",
        "from sklearn.metrics import balanced_accuracy_score, precision_recall_fscore_support, accuracy_score, classification_report\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have three different components for this task:\n",
        "\n",
        "1.   The dataset\n",
        "2.   The tokenizer\n",
        "3.   The model\n",
        "\n",
        "The dataset contains our training and testing data. The tokenizer will convert the dataset into numeric representations of the tokens that will be passed to the model during training. The tokenizer doesn't need to be trained, and is just for preparing the dataset to be passed to the model."
      ],
      "metadata": {
        "id": "PoneB3li-nSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# in this block we are just defining variables that will later be passed to other functions.\n",
        "# first we define the model. the name of the model on the HuggingFace directory\n",
        "modname = \"mlburnham/deberta-v3-base-polistance-affect-v1.0\"\n",
        "# the directory where you want the model checkpoints to save\n",
        "training_directory ='training_base'\n",
        "# use GPU if one is available, else CPU. You will want GPU access for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Device: {device}\")"
      ],
      "metadata": {
        "id": "dK1WhP21ooyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import example train and test data as pandas dataframes. Generally you want a train, validate, and test set\n",
        "train = pd.read_csv('https://raw.githubusercontent.com/MLBurnham/stance_detection_tutorials/main/data/train.csv')\n",
        "validate = pd.read_csv('https://raw.githubusercontent.com/MLBurnham/stance_detection_tutorials/main/data/test.csv')\n",
        "\n",
        "# convert the data to a huggingface dataset for ease of use\n",
        "tr_ds = Dataset.from_pandas(train)\n",
        "val_ds = Dataset.from_pandas(validate)\n",
        "ds = DatasetDict()\n",
        "ds['train'] = tr_ds\n",
        "ds['validate'] = val_ds"
      ],
      "metadata": {
        "id": "sFDvYVkVoqUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(modname)\n",
        "\n",
        "# define a generic tokenizing function\n",
        "def tokenize_function(docs):\n",
        "    return tokenizer(docs['text'], padding = 'max_length', truncation = True)\n",
        "\n",
        "# tokenize the dataset\n",
        "dstok = ds.map(tokenize_function)"
      ],
      "metadata": {
        "id": "mbijbhHposXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import the model. Change the num_labels variable to match the number of classes in your dataset. If the number of labels is different than 2\n",
        "# then the ignore_mismatched_sizes must be true. This tells the model to replace the classifier head of the neural network with a new one that has the appropriate number of labels.\n",
        "# id2label makes sure the model associates 0 with 'not support' and 1 with 'support'. Change this to whatever is appropriate for how many labels you have and what they represent.\n",
        "model = AutoModelForSequenceClassification.from_pretrained(modname, num_labels = 2, ignore_mismatched_sizes=True, id2label = {0:'not support', 1:'support'})"
      ],
      "metadata": {
        "id": "u4cOCzrDov69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've imported the model we can set our training parameters and define how the model will be evaluated during training."
      ],
      "metadata": {
        "id": "GpdDL7DvA2kR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we define all of the training parameters\n",
        "training_args = TrainingArguments(output_dir=training_directory,\n",
        "    logging_dir=f'{training_directory}/logs',\n",
        "    lr_scheduler_type= \"linear\", # The algorithm that will adjust the learning rate while training\n",
        "    group_by_length=False,  # If set to True, can increase speed with dynamic padding, by grouping similar length texts\n",
        "    learning_rate = 2e-5, # the initial learning rate\n",
        "    per_device_train_batch_size = 16, # batch size controls how many documents are passed through the model at once. Higher batch sizes train faster but demand more memory. lower the batch size if you are running out of memory\n",
        "    per_device_eval_batch_size = 16,\n",
        "    gradient_accumulation_steps= 1,  # Number of batches to pass through the model before updating the weights of the neural network. Can be useful when using very small batch sizes like 2 or 4.\n",
        "    num_train_epochs=3, # number of times to pass the entire training set through the model\n",
        "    warmup_ratio=0.06, # warmup length before learning rate scheduler kicks in\n",
        "    weight_decay=0.01, # weight regularization\n",
        "    fp16=True, # the data type that the model's weights are stored in. fp16 stands for floating point 16 and will make the model much smaller and faster.\n",
        "    fp16_full_eval=True,\n",
        "    evaluation_strategy=\"epoch\", # evaluate the model every n steps or epochs.\n",
        "    seed=1,\n",
        "    #eval_steps=50,  # how many steps between evaluations if using steps evaluation strategy. 1 step = 1 gradient update\n",
        "    save_strategy=\"epoch\",  # Save after each epoch or after n steps\n",
        "    #save_steps=100,  # Number of updates steps before two checkpoint saves.\n",
        "    dataloader_num_workers = 1, # number of cpu workers passing data to the the GPU\n",
        ")"
      ],
      "metadata": {
        "id": "NLfaXdgJowb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is a custom function that can be passed to the trainer and will report a battery of metrics to report while training."
      ],
      "metadata": {
        "id": "GX_culP-BH8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this function will be used to calculate performance metrics during training\n",
        "def compute_metrics(eval_pred, label_text_alphabetical=list(model.config.id2label.values())):\n",
        "    # Extract labels\n",
        "    labels = eval_pred.label_ids\n",
        "    pred_logits = eval_pred.predictions\n",
        "    preds_max = np.argmax(pred_logits, axis=1)\n",
        "\n",
        "    # Compute the metrics\n",
        "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(labels, preds_max, average='macro')\n",
        "    precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(labels, preds_max, average='micro')\n",
        "    acc_balanced = balanced_accuracy_score(labels, preds_max)\n",
        "    acc_not_balanced = accuracy_score(labels, preds_max)\n",
        "\n",
        "    # Pass computed metrics to a dictionary for printing\n",
        "    metrics = {'f1_macro': f1_macro,\n",
        "            'f1_micro': f1_micro,\n",
        "            'accuracy_balanced': acc_balanced,\n",
        "            'accuracy': acc_not_balanced,\n",
        "            'precision_macro': precision_macro,\n",
        "            'recall_macro': recall_macro,\n",
        "            'precision_micro': precision_micro,\n",
        "            'recall_micro': recall_micro,\n",
        "            }\n",
        "\n",
        "    # Print results\n",
        "    print(\"Aggregate metrics: \", {key: metrics[key] for key in metrics if key not in [\"label_gold_raw\", \"label_predicted_raw\"]} )\n",
        "    print(\"Detailed metrics: \", classification_report(\n",
        "        labels, preds_max, labels=np.sort(pd.factorize(label_text_alphabetical, sort=True)[0]),\n",
        "        target_names=label_text_alphabetical, sample_weight=None,\n",
        "        digits=2, output_dict=True, zero_division='warn'),\n",
        "    \"\\n\")\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "IZ4vaVSwoygg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've prepared everything, we just pass the model, tokenizer, dataset, training parameters, and metrics function to the trainer. Then we simply call the trainer to start the process."
      ],
      "metadata": {
        "id": "pDMOBhjsD3wd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate the trainer by passing our model, tokenizer, training parameters, data, and metrics function to it\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    train_dataset=dstok['train'],\n",
        "    eval_dataset=dstok['validate'],\n",
        "    compute_metrics=lambda x: compute_metrics(x, label_text_alphabetical=list(model.config.id2label.values()))\n",
        ")"
      ],
      "metadata": {
        "id": "1uObSYvJo0WX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# call the trainer to train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "FqypPxUGo2TC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}