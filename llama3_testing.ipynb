{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2a2297d-ad90-4a4b-a0c9-01cb3028469c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6a7c9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "motn = pd.read_csv('data/motn_test_text.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadd84f3-2293-426c-8f15-364962a1e9eb",
   "metadata": {},
   "source": [
    "# Llama 3 8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e4271dc-de75-4376-bad8-a0101ce39b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikeb\\.conda\\envs\\sandbox\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af2192fcb3f04ec59a41ff5533b0f579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "pipe = pipeline(\"text-generation\", model=model, torch_dtype=torch.bfloat16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380f9b5d-6089-40cd-b83a-1eaaad38f0a4",
   "metadata": {},
   "source": [
    "No Logit Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4baa71cc-5910-4303-a21d-b9161551cb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#system_message =  \"\"\"You are a text classifier and are only allowed to respond with a 1 or a 0.\"\"\"\n",
    "user_message = \"\"\"You are a classifier that determines what category a survey response belongs to. This survey asked people to respond to the question: \"What does democracy mean to you?\" Each answer belongs to one of the following 12 categories:\n",
    "\n",
    "1. Freedom and Rights\n",
    "2. One United Nation\n",
    "3. Representation of the People\n",
    "4. Popular Will and Equality\n",
    "5. National Identity and Heritage\n",
    "6. Not a Democracy, a Republic\n",
    "7. Flawed Democracy\n",
    "8. Institution and Constitution\n",
    "9. Disaffected\n",
    "10. I don't know\n",
    "11. Nothing\n",
    "12. Other\n",
    "\n",
    "I will show you an answer to the question. Read the answer and then determine which of the categories most closely represents the respondent's answer. Return the number of the most appropriate category. Do not explain your answer, and only return the number. Here is the respondent's answer:\n",
    "{}\n",
    "\n",
    "Which category number best describes this answer?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "513b7a34-108f-423d-a2b4-05f8c8764fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikeb\\.conda\\envs\\sandbox\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mikeb\\.conda\\envs\\sandbox\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mikeb\\.conda\\envs\\sandbox\\Lib\\site-packages\\transformers\\pipelines\\base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'4', '7', '1', '6', '2', 'I cannot', '5', '10', '9', '11', '8', '3'}\n",
      "CPU times: total: 1min 47s\n",
      "Wall time: 1min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res = []\n",
    "for doc in motn['comment_text']:\n",
    "    messages = [\n",
    "        #{\n",
    "            #\"role\": \"system\",\n",
    "            #\"content\": system_message,\n",
    "        #},\n",
    "        {\"role\": \"user\", \"content\": user_message.format(doc)},\n",
    "    ]\n",
    "    prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    outputs = pipe(prompt, max_new_tokens=2, do_sample=False, return_full_text = False, pad_token_id=pipe.tokenizer.eos_token_id, temperature = 0)\n",
    "    res.extend(outputs)\n",
    "\n",
    "res = [text['generated_text'] for text in res]\n",
    "\n",
    "# return a list of unique responses from the model\n",
    "print(set(res))\n",
    "\n",
    "# add results to the dataframe\n",
    "#motn['llama'] = [1 if '1' in text else 0 for text in res]\n",
    "# export results\n",
    "#motn.to_csv('./motn_test_llama3.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7d3783a-fd97-4e5c-bcd7-36a0dff2fc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "motn['llama'] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b5248cf-b3ab-4e08-ad47-f2806c508e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "motn.to_csv('llama_test.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
